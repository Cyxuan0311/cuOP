# cuOP Python API

cuOP Python API æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„CUDAç®—å­å’Œå†…å­˜ç®¡ç†åº“ï¼Œæä¾›JITä¼˜åŒ–ã€æŒä¹…åŒ–ç¼“å­˜å’Œé«˜æ•ˆçš„å†…å­˜ç®¡ç†åŠŸèƒ½ã€‚

## ç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½CUDAç®—å­**: æ”¯æŒGEMMã€GEMVã€ReLUã€Softmaxã€MatMulç­‰å¸¸ç”¨ç®—å­
- âš¡ **JITä¼˜åŒ–**: å®æ—¶å†…æ ¸ä¼˜åŒ–ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³é…ç½®
- ğŸ’¾ **æŒä¹…åŒ–ç¼“å­˜**: ç¼–è¯‘ç»“æœæŒä¹…åŒ–ï¼Œæ˜¾è‘—æå‡é‡å¤ä½¿ç”¨æ€§èƒ½
- ğŸ§  **æ™ºèƒ½å†…å­˜ç®¡ç†**: é«˜æ•ˆçš„å†…å­˜æ± å’Œè‡ªåŠ¨å†…å­˜ä¼˜åŒ–
- ğŸ”§ **çµæ´»é…ç½®**: å¯è‡ªå®šä¹‰JITé…ç½®å’Œä¼˜åŒ–å‚æ•°
- ğŸ“Š **æ€§èƒ½åˆ†æ**: å†…ç½®æ€§èƒ½åˆ†æå’ŒåŸºå‡†æµ‹è¯•å·¥å…·
- ğŸ **PythonåŸç”Ÿ**: å®Œå…¨PythonåŒ–çš„APIè®¾è®¡ï¼Œæ˜“äºä½¿ç”¨

## å®‰è£…

### ç³»ç»Ÿè¦æ±‚

- Python 3.7+
- CUDA 11.0+
- NVIDIA GPU (æ”¯æŒCUDA)
- Linux/macOS/Windows

### ä»æºç å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/cuop/cuop.git
cd cuop

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# æ„å»ºPythonåŒ…
cd python
pip install -e .
```

### ä¾èµ–å®‰è£…

```bash
pip install numpy pybind11
```

## å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```python
import cuop
import numpy as np

# åˆ›å»ºå¼ é‡
a = cuop.tensor(np.random.randn(1000, 1000).astype(np.float32))
b = cuop.tensor(np.random.randn(1000, 1000).astype(np.float32))
c = cuop.zeros([1000, 1000])

# ä½¿ç”¨GEMMç®—å­
gemm = cuop.GemmFloat()
gemm.set_weight(b)
gemm.forward(a, c)

# è½¬æ¢ä¸ºnumpyæ•°ç»„
result = c.to_numpy()
```

### cuBlasç®—å­ä½¿ç”¨

```python
# å‘é‡è¿ç®—
x = cuop.tensor(np.random.randn(1000).astype(np.float32))
y = cuop.tensor(np.random.randn(1000).astype(np.float32))

# SCAL: æ ‡é‡å‘é‡ä¹˜æ³•
scal = cuop.ScalFloat(2.0)
scal.forward(x)  # x = 2.0 * x

# AXPY: å‘é‡åŠ æ³•
axpy = cuop.AxpyFloat(1.5)
axpy.forward(x, y)  # y = 1.5 * x + y

# COPY: å‘é‡å¤åˆ¶
copy = cuop.CopyFloat()
copy.forward(x, y)  # y = x

# DOT: å‘é‡ç‚¹ç§¯
dot = cuop.DotFloat()
result = dot.forward(x, y)  # result = x^T * y

# çŸ©é˜µè¿ç®—
A = cuop.tensor(np.random.randn(1000, 1000).astype(np.float32))
B = cuop.tensor(np.random.randn(1000, 1000).astype(np.float32))
C = cuop.zeros([1000, 1000])

# SYMM: å¯¹ç§°çŸ©é˜µä¹˜æ³•
symm = cuop.SymmFloat(0, 1, 1.0, 0.0)  # side=left, uplo=upper, alpha=1.0, beta=0.0
symm.set_weight(A)
symm.forward(B, C)  # C = A * B (Aä¸ºå¯¹ç§°çŸ©é˜µ)

# TRSM: ä¸‰è§’çŸ©é˜µæ±‚è§£
trsm = cuop.TrsmFloat(0, 1, 0, 0, 1.0)  # left, lower, non-trans, non-unit, alpha=1.0
trsm.set_matrix_a(A)
trsm.forward(B, B)  # æ±‚è§£ A * X = Bï¼Œç»“æœå­˜å‚¨åœ¨Bä¸­
```

### cuDNNç®—å­ä½¿ç”¨

```python
# æ¿€æ´»å‡½æ•°
input_tensor = cuop.tensor(np.random.randn(1000, 1000).astype(np.float32))
output_tensor = cuop.zeros([1000, 1000])

# ReLUæ¿€æ´»
relu = cuop.ReluFloat()
relu.forward(input_tensor, output_tensor)

# Softmaxå½’ä¸€åŒ–
softmax = cuop.SoftmaxFloat()
softmax.forward(input_tensor, output_tensor, axis=-1)  # åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šè®¡ç®—softmax

# å½’ä¸€åŒ–å±‚
batch_size, channels, height, width = 32, 64, 224, 224
input_4d = cuop.tensor(np.random.randn(batch_size, channels, height, width).astype(np.float32))
output_4d = cuop.zeros([batch_size, channels, height, width])

# BatchNormæ‰¹å½’ä¸€åŒ–
batchnorm = cuop.BatchNormFloat()
gamma = cuop.ones([channels])
beta = cuop.zeros([channels])
running_mean = cuop.zeros([channels])
running_var = cuop.ones([channels])
batchnorm.set_gamma(gamma)
batchnorm.set_beta(beta)
batchnorm.set_running_mean(running_mean)
batchnorm.set_running_var(running_var)
batchnorm.forward(input_4d, output_4d)

# LayerNormå±‚å½’ä¸€åŒ–
layernorm = cuop.LayerNormFloat()
layernorm.set_gamma(gamma)
layernorm.set_beta(beta)
layernorm.forward(input_4d, output_4d)

# å·ç§¯å±‚
in_channels, out_channels = 64, 128
kernel_h, kernel_w = 3, 3
stride_h, stride_w = 1, 1
conv = cuop.Convolution2DFloat(in_channels, out_channels, kernel_h, kernel_w, stride_h, stride_w)
weight = cuop.tensor(np.random.randn(out_channels, in_channels, kernel_h, kernel_w).astype(np.float32))
bias = cuop.tensor(np.random.randn(out_channels).astype(np.float32))
conv.set_weight(weight)
conv.set_bias(bias)
conv_output = cuop.zeros([batch_size, out_channels, height, width])
conv.forward(input_4d, conv_output, pad_h=1, pad_w=1)

# æ± åŒ–å±‚
pool_input = cuop.tensor(np.random.randn(32, 64, 56, 56).astype(np.float32))
pool_output = cuop.zeros([32, 64, 28, 28])

# MaxPool2D
maxpool = cuop.MaxPool2DFloat()
maxpool.forward(pool_input, pool_output, kernel_h=2, kernel_w=2)

# AveragePool2D
avgpool = cuop.AveragePool2DFloat()
avgpool.forward(pool_input, pool_output, kernel_h=2, kernel_w=2)

# å¼ é‡æ“ä½œ
# Flatten
flatten = cuop.FlattenFloat()
flatten_output = cuop.zeros([32, 64*56*56])
flatten.forward(pool_input, flatten_output, start_dim=1)

# View
view = cuop.ViewFloat()
view_output = cuop.zeros([32, 64, 56, 56])
view.set_offset([0, 0, 0, 0])
view.set_shape([32, 64, 56, 56])
view.forward(pool_input, view_output, [0, 0, 0, 0], [32, 64, 56, 56])
```

### JITä¼˜åŒ–ä½¿ç”¨

```python
# åˆ›å»ºJITä¼˜åŒ–çš„ç®—å­
jit_gemm = cuop.JITGemmFloat(gemm)

# å¯ç”¨JITä¼˜åŒ–
jit_gemm.enable_jit(True)

# å¯ç”¨æŒä¹…åŒ–ç¼“å­˜
jit_gemm.enable_persistent_cache(True)
jit_gemm.set_persistent_cache_directory("./jit_cache")

# è®¾ç½®JITé…ç½®
config = cuop.get_default_jit_config()
config.kernel_type = "tiled"
config.tile_size = 32
config.block_size = 256
config.optimization_level = "O2"
config.enable_tensor_core = True

jit_gemm.set_jit_config(config)

# æ‰§è¡Œè®¡ç®—
jit_gemm.forward(a, c)

# è·å–æ€§èƒ½ä¿¡æ¯
profile = jit_gemm.get_performance_profile()
print(f"æ‰§è¡Œæ—¶é—´: {profile.execution_time}ms")
print(f"GFLOPS: {profile.gflops}")
```

### æ‰¹é‡å¤„ç†

```python
# åˆ›å»ºæ‰¹é‡æ•°æ®
batch_size = 100
matrix_size = 256
batch_inputs = []
batch_weights = []

for i in range(batch_size):
    input_data = np.random.randn(matrix_size, matrix_size).astype(np.float32)
    weight_data = np.random.randn(matrix_size, matrix_size).astype(np.float32)
    
    batch_inputs.append(cuop.tensor(input_data))
    batch_weights.append(cuop.tensor(weight_data))

# æ‰¹é‡å¤„ç†
gemm = cuop.GemmFloat()
batch_outputs = [cuop.zeros([matrix_size, matrix_size]) for _ in range(batch_size)]

for i in range(batch_size):
    gemm.set_weight(batch_weights[i])
    gemm.forward(batch_inputs[i], batch_outputs[i])

cuop.synchronize()
```

## APIå‚è€ƒ

### æ ¸å¿ƒç±»

#### Tensor

å¼ é‡ç±»ï¼Œæ”¯æŒå¤šç§æ•°æ®ç±»å‹å’Œå½¢çŠ¶ã€‚

```python
# åˆ›å»ºå¼ é‡
tensor = cuop.TensorFloat([1000, 1000])
tensor = cuop.tensor(np.random.randn(1000, 1000).astype(np.float32))

# å¼ é‡æ“ä½œ
tensor.fill(1.0)           # å¡«å……å€¼
tensor.zero()              # æ¸…é›¶
tensor.copy_from(arr)      # ä»numpyæ•°ç»„å¤åˆ¶
result = tensor.to_numpy() # è½¬æ¢ä¸ºnumpyæ•°ç»„

# å±æ€§
shape = tensor.shape       # è·å–å½¢çŠ¶
size = tensor.size         # è·å–å…ƒç´ æ•°é‡
dtype = tensor.dtype       # è·å–æ•°æ®ç±»å‹
```

#### ç®—å­ç±»

æ”¯æŒå¤šç§CUDAç®—å­ï¼š

**cuBlasç®—å­ï¼š**
- `ScalFloat/ScalDouble`: æ ‡é‡å‘é‡ä¹˜æ³•
- `AxpyFloat/AxpyDouble`: å‘é‡åŠ æ³•
- `CopyFloat/CopyDouble`: å‘é‡å¤åˆ¶
- `DotFloat/DotDouble`: å‘é‡ç‚¹ç§¯
- `GemmFloat/GemmDouble`: é€šç”¨çŸ©é˜µä¹˜æ³•
- `GemvFloat/GemvDouble`: çŸ©é˜µå‘é‡ä¹˜æ³•
- `SymmFloat/SymmDouble`: å¯¹ç§°çŸ©é˜µä¹˜æ³•
- `TrsmFloat/TrsmDouble`: ä¸‰è§’çŸ©é˜µæ±‚è§£

**cuDNNç®—å­ï¼š**
- `ReluFloat/ReluDouble`: ReLUæ¿€æ´»å‡½æ•°
- `SoftmaxFloat/SoftmaxDouble`: Softmaxå½’ä¸€åŒ–
- `BatchNormFloat/BatchNormDouble`: æ‰¹å½’ä¸€åŒ–
- `LayerNormFloat/LayerNormDouble`: å±‚å½’ä¸€åŒ–
- `Convolution2DFloat/Convolution2DDouble`: 2Då·ç§¯
- `MatMulFloat/MatMulDouble`: çŸ©é˜µä¹˜æ³•
- `BatchMatMulFloat/BatchMatMulDouble`: æ‰¹é‡çŸ©é˜µä¹˜æ³•
- `FlattenFloat/FlattenDouble`: å¼ é‡å±•å¹³
- `ViewFloat/ViewDouble`: å¼ é‡è§†å›¾
- `MaxPool2DFloat/MaxPool2DDouble`: 2Dæœ€å¤§æ± åŒ–
- `AveragePool2DFloat/AveragePool2DDouble`: 2Då¹³å‡æ± åŒ–
- `GlobalMaxPool2DFloat/GlobalMaxPool2DDouble`: å…¨å±€æœ€å¤§æ± åŒ–
- `GlobalAveragePool2DFloat/GlobalAveragePool2DDouble`: å…¨å±€å¹³å‡æ± åŒ–

```python
# åˆ›å»ºcuBlasç®—å­
gemm = cuop.GemmFloat()
gemm.set_weight(weight_tensor)
gemm.forward(input_tensor, output_tensor)

# åˆ›å»ºcuDNNç®—å­
relu = cuop.ReluFloat()
relu.forward(input_tensor, output_tensor)

# åˆ›å»ºå½’ä¸€åŒ–ç®—å­
batchnorm = cuop.BatchNormFloat()
batchnorm.set_gamma(gamma_tensor)
batchnorm.set_beta(beta_tensor)
batchnorm.forward(input_tensor, output_tensor)
```

#### JITåŒ…è£…å™¨

JITä¼˜åŒ–çš„ç®—å­åŒ…è£…å™¨ï¼Œæ”¯æŒæ‰€æœ‰ä¸»è¦ç®—å­ï¼š

**æ”¯æŒçš„JITåŒ…è£…å™¨ï¼š**
- `JITGemmFloat/JITGemmDouble`: JITä¼˜åŒ–çš„çŸ©é˜µä¹˜æ³•
- `JITGemvFloat/JITGemvDouble`: JITä¼˜åŒ–çš„çŸ©é˜µå‘é‡ä¹˜æ³•
- `JITReluFloat/JITReluDouble`: JITä¼˜åŒ–çš„ReLUæ¿€æ´»
- `JITSoftmaxFloat/JITSoftmaxDouble`: JITä¼˜åŒ–çš„Softmaxå½’ä¸€åŒ–
- `JITBatchNormFloat/JITBatchNormDouble`: JITä¼˜åŒ–çš„æ‰¹å½’ä¸€åŒ–
- `JITLayerNormFloat/JITLayerNormDouble`: JITä¼˜åŒ–çš„å±‚å½’ä¸€åŒ–
- `JITConvolution2DFloat/JITConvolution2DDouble`: JITä¼˜åŒ–çš„2Då·ç§¯
- `JITMatMulFloat/JITMatMulDouble`: JITä¼˜åŒ–çš„çŸ©é˜µä¹˜æ³•

```python
# åˆ›å»ºåŸºç¡€ç®—å­
gemm = cuop.GemmFloat()
relu = cuop.ReluFloat()
batchnorm = cuop.BatchNormFloat()

# åˆ›å»ºJITåŒ…è£…å™¨
jit_gemm = cuop.JITGemmFloat(gemm)
jit_relu = cuop.JITReluFloat(relu)
jit_batchnorm = cuop.JITBatchNormFloat(batchnorm)

# é…ç½®JIT
jit_gemm.enable_jit(True)
jit_gemm.enable_persistent_cache(True)
jit_gemm.set_persistent_cache_directory("./jit_cache")

# è®¾ç½®é…ç½®
config = cuop.JITConfig()
config.kernel_type = "tiled"
config.tile_size = 32
config.block_size = 256
config.enable_tensor_core = True
jit_gemm.set_jit_config(config)

# æ‰§è¡Œ
jit_gemm.forward(input_tensor, output_tensor)
jit_relu.forward(input_tensor, output_tensor)
jit_batchnorm.forward(input_tensor, output_tensor)

# è·å–æ€§èƒ½ä¿¡æ¯
profile = jit_gemm.get_performance_profile()
print(f"æ‰§è¡Œæ—¶é—´: {profile.execution_time}ms")
print(f"GFLOPS: {profile.gflops}")
```

### é…ç½®ç±»

#### JITConfig

JITç¼–è¯‘é…ç½®ï¼š

```python
config = cuop.JITConfig()
config.enable_jit = True
config.kernel_type = "tiled"           # å†…æ ¸ç±»å‹
config.tile_size = 32                  # ç“¦ç‰‡å¤§å°
config.block_size = 256                # å—å¤§å°
config.optimization_level = "O2"       # ä¼˜åŒ–çº§åˆ«
config.enable_tensor_core = True       # å¯ç”¨Tensor Core
config.enable_tma = True               # å¯ç”¨TMA
config.max_registers = 255             # æœ€å¤§å¯„å­˜å™¨æ•°
config.enable_shared_memory_opt = True # å¯ç”¨å…±äº«å†…å­˜ä¼˜åŒ–
config.enable_loop_unroll = True       # å¯ç”¨å¾ªç¯å±•å¼€
config.enable_memory_coalescing = True # å¯ç”¨å†…å­˜åˆå¹¶
```

#### GlobalJITConfig

å…¨å±€JITé…ç½®ï¼š

```python
global_config = cuop.GlobalJITConfig()
global_config.enable_jit = True
global_config.enable_auto_tuning = True
global_config.enable_caching = True
global_config.cache_dir = "./jit_cache"
global_config.max_cache_size = 10 * 1024 * 1024 * 1024  # 10GB
global_config.compilation_timeout = 30
global_config.max_compilation_threads = 4
```

### å·¥å…·å‡½æ•°

#### å¼ é‡åˆ›å»º

```python
# åˆ›å»ºå¼ é‡
tensor = cuop.tensor(numpy_array)
zeros = cuop.zeros(shape)
ones = cuop.ones(shape)
random_tensor = cuop.random(shape, dtype='float32', mean=0.0, std=1.0)

# ç‰¹æ®Šå¼ é‡
identity = cuop.eye(n, dtype='float32')
linspace_tensor = cuop.linspace(start, stop, num, dtype='float32')
logspace_tensor = cuop.logspace(start, stop, num, base=10.0, dtype='float32')
grid_tensors = cuop.meshgrid(*xi, indexing='xy')
```

#### è®¾å¤‡ç®¡ç†

```python
# è®¾å¤‡ä¿¡æ¯
device_count = cuop.get_device_count()
current_device = cuop.get_device()

# è®¾å¤‡æ§åˆ¶
cuop.set_device(device_id)
cuop.synchronize()

# å†…å­˜ä¿¡æ¯
free_mem, total_mem = cuop.get_memory_info()
cuop.empty_cache()
```

#### æ€§èƒ½åˆ†æ

```python
# åŸºå‡†æµ‹è¯•
stats = cuop.benchmark(operator, input_tensor, output_tensor, 
                       num_runs=100, warmup_runs=10)

print(f"å¹³å‡æ—¶é—´: {stats['mean_time_ms']:.3f} ms")
print(f"æ ‡å‡†å·®: {stats['std_time_ms']:.3f} ms")
print(f"æœ€å°æ—¶é—´: {stats['min_time_ms']:.3f} ms")
print(f"æœ€å¤§æ—¶é—´: {stats['max_time_ms']:.3f} ms")
```

## æœ€ä½³å®è·µ

### 1. å†…å­˜ç®¡ç†

```python
# åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¼ é‡
del large_tensor
cuop.empty_cache()

# ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç®¡ç†å†…å­˜
with cuop.memory_context():
    # åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­åˆ›å»ºçš„å¼ é‡ä¼šè‡ªåŠ¨ç®¡ç†
    tensor = cuop.tensor(data)
    # ä½¿ç”¨tensor...
# ç¦»å¼€ä¸Šä¸‹æ–‡åè‡ªåŠ¨æ¸…ç†
```

### 2. JITé…ç½®ä¼˜åŒ–

```python
# æ ¹æ®æ•°æ®å¤§å°é€‰æ‹©é…ç½®
if matrix_size < 1024:
    config.kernel_type = "simple"
    config.tile_size = 16
elif matrix_size < 4096:
    config.kernel_type = "tiled"
    config.tile_size = 32
else:
    config.kernel_type = "tensor_core"
    config.tile_size = 64
```

### 3. æ‰¹é‡å¤„ç†

```python
# ä½¿ç”¨æ‰¹é‡å¤„ç†æé«˜æ•ˆç‡
batch_size = 100
for i in range(0, total_size, batch_size):
    batch = data[i:i+batch_size]
    # å¤„ç†æ‰¹æ¬¡...
    cuop.synchronize()  # å®šæœŸåŒæ­¥
```

### 4. é”™è¯¯å¤„ç†

```python
try:
    result = operator.forward(input, output)
except cuop.MemoryError as e:
    print(f"å†…å­˜ä¸è¶³: {e}")
    cuop.empty_cache()
except cuop.ExecutionError as e:
    print(f"æ‰§è¡Œé”™è¯¯: {e}")
except Exception as e:
    print(f"æœªçŸ¥é”™è¯¯: {e}")
```

## æ€§èƒ½ä¼˜åŒ–

### 1. é€‰æ‹©åˆé€‚çš„ç®—å­

- å°çŸ©é˜µ (< 512x512): ä½¿ç”¨åŸºç¡€ç®—å­
- ä¸­ç­‰çŸ©é˜µ (512x512 - 4096x4096): ä½¿ç”¨JITä¼˜åŒ–ç®—å­
- å¤§çŸ©é˜µ (> 4096x4096): ä½¿ç”¨Tensor Coreä¼˜åŒ–

### 2. å†…å­˜è®¿é—®ä¼˜åŒ–

- ä½¿ç”¨è¿ç»­çš„å†…å­˜å¸ƒå±€
- é¿å…é¢‘ç¹çš„å†…å­˜åˆ†é…/é‡Šæ”¾
- åˆ©ç”¨å…±äº«å†…å­˜ä¼˜åŒ–

### 3. å¹¶è¡ŒåŒ–ç­–ç•¥

- æ ¹æ®GPUæ¶æ„è°ƒæ•´å—å¤§å°
- ä½¿ç”¨é€‚å½“çš„ç“¦ç‰‡å¤§å°
- å¯ç”¨å¾ªç¯å±•å¼€å’Œå‘é‡åŒ–

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **CUDAé”™è¯¯**
   ```python
   # æ£€æŸ¥CUDAç¯å¢ƒ
   cuop.print_system_info()
   
   # æ£€æŸ¥è®¾å¤‡çŠ¶æ€
   device_count = cuop.get_device_count()
   current_device = cuop.get_device()
   ```

2. **å†…å­˜ä¸è¶³**
   ```python
   # æ¸…ç†ç¼“å­˜
   cuop.empty_cache()
   
   # æ£€æŸ¥å†…å­˜ä½¿ç”¨
   free_mem, total_mem = cuop.get_memory_info()
   ```

3. **ç¼–è¯‘å¤±è´¥**
   ```python
   # æ£€æŸ¥JITé…ç½®
   config = cuop.get_default_jit_config()
   
   # å°è¯•ä¸åŒçš„ä¼˜åŒ–çº§åˆ«
   config.optimization_level = "O1"  # é™ä½ä¼˜åŒ–çº§åˆ«
   ```

### è°ƒè¯•æ¨¡å¼

```python
# å¯ç”¨è°ƒè¯•æ¨¡å¼
global_config = cuop.get_default_global_jit_config()
global_config.enable_debug = True

# è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯
error_msg = operator.get_last_error()
```

## ç¤ºä¾‹ä»£ç 

å®Œæ•´çš„ç¤ºä¾‹ä»£ç è¯·å‚è€ƒï¼š

- `examples/quick_start.py`: å¿«é€Ÿå¼€å§‹ç¤ºä¾‹
- `examples/complete_operator_demo.py`: å®Œæ•´ç®—å­æ¼”ç¤º

## è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·å‚è€ƒ [CONTRIBUTING.md](../CONTRIBUTING.md) äº†è§£è´¡çŒ®æŒ‡å—ã€‚

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ï¼Œè¯¦è§ [LICENSE](../LICENSE) æ–‡ä»¶ã€‚

## æ”¯æŒ

å¦‚æœæ‚¨é‡åˆ°é—®é¢˜æˆ–æœ‰å»ºè®®ï¼Œè¯·ï¼š

1. æŸ¥çœ‹ [æ–‡æ¡£](https://cuop.readthedocs.io/)
2. æœç´¢ [Issues](https://github.com/cuop/cuop/issues)
3. åˆ›å»ºæ–°çš„ [Issue](https://github.com/cuop/cuop/issues/new)
4. åŠ å…¥ [Discussions](https://github.com/cuop/cuop/discussions)

## æ›´æ–°æ—¥å¿—

### v0.1.0
- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- æ”¯æŒåŸºç¡€CUDAç®—å­
- JITä¼˜åŒ–ç³»ç»Ÿ
- æŒä¹…åŒ–ç¼“å­˜
- Python API
